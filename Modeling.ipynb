{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda/envs/py36/lib/python3.6/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import mm_predictor # stat-analysis library\n",
    "from sklearn import cross_validation, linear_model\n",
    "import csv\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.pipeline import make_pipeline, make_union\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.svm import LinearSVC\n",
    "from tpot.builtins import StackingEstimator\n",
    "from xgboost import XGBClassifier\n",
    "from tpot import TPOTClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# intialize stat & elo dictionaries\n",
    "mm_predictor.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load data\n",
    "season_data = pd.read_csv('./ncaa-data/RegularSeasonDetailedResults.csv')\n",
    "tourney_data = pd.read_csv('./ncaa-data/NCAATourneyDetailedResults.csv')\n",
    "tourney_data = tourney_data[tourney_data.Season != 2017]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "aggregated_data = pd.concat([season_data, tourney_data])\n",
    "\n",
    "X,Y = mm_predictor.analyze_teams(aggregated_data)\n",
    "# Run script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda/envs/py36/lib/python3.6/site-packages/sklearn/decomposition/fastica_.py:118: UserWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.\n",
      "  warnings.warn('FastICA did not converge. Consider increasing '\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('fastica-1', FastICA(algorithm='parallel', fun='logcosh', fun_args=None, max_iter=200,\n",
       "    n_components=None, random_state=None, tol=0.0, w_init=None,\n",
       "    whiten=True)), ('fastica-2', FastICA(algorithm='parallel', fun='logcosh', fun_args=None, max_iter=200,\n",
       "    n_components=None, random_state=None, tol=0.0, w_init=None,\n",
       "    whiten=True)), ('normalizer', Normalizer(copy=True, norm='l2')), ('gaussiannb', GaussianNB(priors=None))])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.pipeline import make_pipeline, make_union\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.svm import LinearSVC\n",
    "from tpot.builtins import StackingEstimator\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.decomposition import FastICA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import Normalizer\n",
    "\n",
    "# Score on the training set was:-0.5443811951009836\n",
    "# 'log loss' pipeline\n",
    "exported_pipeline = make_pipeline(\n",
    "    StackingEstimator(estimator=XGBClassifier(learning_rate=0.01, max_depth=2, min_child_weight=4, n_estimators=100, nthread=1, subsample=0.15000000000000002)),\n",
    "    RobustScaler(),\n",
    "    StackingEstimator(estimator=GaussianNB()),\n",
    "    StackingEstimator(estimator=LinearSVC(C=15.0, dual=True, loss=\"hinge\", penalty=\"l2\", tol=0.0001)),\n",
    "    StackingEstimator(estimator=GaussianNB()),\n",
    "    LogisticRegression(C=5.0, dual=False, penalty=\"l1\")\n",
    ")\n",
    "\n",
    "# exported_pipeline = RandomForestClassifier(bootstrap=True, criterion=\"gini\", max_features=0.55, min_samples_leaf=5, min_samples_split=9, n_estimators=100)\n",
    "\n",
    "# exported_pipeline = DecisionTreeClassifier(criterion=\"gini\", max_depth=7, min_samples_leaf=11, min_samples_split=12)\n",
    "\n",
    "# exported_pipeline.fit(training_features, training_target)\n",
    "\n",
    "#exported_pipeline = LogisticRegression()\n",
    "\n",
    "# 'accuracy' pipeline\n",
    "exported_pipeline = make_pipeline(\n",
    "    FastICA(tol=0.0),\n",
    "    FastICA(tol=0.0),\n",
    "    Normalizer(norm=\"l2\"),\n",
    "    GaussianNB()\n",
    ")\n",
    "\n",
    "exported_pipeline.fit(np.array(X), np.array(Y))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create id to team name mappings\n",
    "teamNames = pd.read_csv('./ncaa-data/Teams.csv')\n",
    "def teamIDNameMapping():\n",
    "    team_id_map = {}\n",
    "    for index, row in teamNames.iterrows():\n",
    "        team_id_map[row['TeamID']] = row['TeamName']\n",
    "    \n",
    "    return team_id_map\n",
    "\n",
    "team_id_map = teamIDNameMapping()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def WinChanceMapping(year, exported_pipeline):\n",
    "    \n",
    "    submission_data = []\n",
    "    tourney_seeds = pd.read_csv('./ncaa-data/TourneySeeds.csv')\n",
    "    tourney_teams = tourney_seeds[tourney_seeds['Season'] == year]['Team']\n",
    "\n",
    "#     stat_fields = ['score', 'OR', 'DR', 'Ast', 'TO', 'Stl', 'Blk', 'PF',\n",
    "#                    'fgp', '3pp', 'ftp']\n",
    "    stat_fields = ['score', 'fga', 'fgp', 'fga3', '3pp', 'ftp', 'or', 'dr',\n",
    "                   'ast', 'to', 'stl', 'blk', 'pf']\n",
    "    \n",
    "    tourney_teams.sort_values()\n",
    "    \n",
    "    for team_1 in tourney_teams:\n",
    "        for team_2 in tourney_teams:\n",
    "            if team_1 != team_2:\n",
    "                prediction = mm_predictor.predict_winner(\n",
    "                    team_1, team_2, exported_pipeline, 2017, stat_fields)\n",
    "                label = '2017_' + str(team_1) + '_' + str(team_2)\n",
    "                submission_data.append([label, prediction[0][0]])\n",
    "    \n",
    "    \n",
    "    teamTeamWinChanceMap = {}\n",
    "    for line in submission_data:\n",
    "        title = line[0].split('_')\n",
    "        team1 = team_id_map[int(title[1])]\n",
    "        team2 = team_id_map[int(title[2])]\n",
    "        winRate = str(round(line[1]* 100,2))\n",
    "        if team1 not in teamTeamWinChanceMap:\n",
    "            teamTeamWinChanceMap[team1] = {}\n",
    "        if float(winRate) >= 50:\n",
    "            if team1 in teamTeamWinChanceMap and team2 in teamTeamWinChanceMap[team1] and teamTeamWinChanceMap[team1][team2] > float(winRate):\n",
    "                continue\n",
    "            if team2 in teamTeamWinChanceMap and team1 in teamTeamWinChanceMap[team2] and teamTeamWinChanceMap[team2][team1] < float(winRate):\n",
    "                # update teamTeammap[team2][team1] to remove that key\n",
    "                teamTeamWinChanceMap[team2].pop(team1, None)\n",
    "            teamTeamWinChanceMap[team1][team2] = float(winRate)\n",
    "            \n",
    "    return teamTeamWinChanceMap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# print(\"Writing %d results.\" % len(submission_data))\n",
    "# with open('./ncaa-data/submission.csv', 'w') as f:\n",
    "#     writer = csv.writer(f)\n",
    "#     writer.writerow(['id', 'pred'])\n",
    "#     writer.writerows(submission_data)\n",
    "\n",
    "chanceMap = WinChanceMapping(2017, exported_pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def filterTourneyDataByYear(year):\n",
    "\n",
    "    tourney_data = pd.read_csv('./ncaa-data/NCAATourneyDetailedResults.csv')\n",
    "    tourney_data = tourney_data[tourney_data.Season == year]\n",
    "    \n",
    "    return tourney_data\n",
    "\n",
    "def getWinnersList(tourney_data):\n",
    "    winners = []\n",
    "\n",
    "    tourney_data_grouped = tourney_data[4:].groupby('WTeamID').size().reset_index(name='NumWins')\n",
    "    for index, row in tourney_data_grouped.iterrows():\n",
    "        teamName = team_id_map[row['WTeamID']]\n",
    "        wins = row['NumWins']\n",
    "        if len(winners) == 0:\n",
    "            winners.append([])\n",
    "        winners[0].append(teamName)\n",
    "        if wins > 1:\n",
    "            if len(winners) == 1:\n",
    "                winners.append([])\n",
    "            winners[1].append(teamName)\n",
    "        if wins > 2:\n",
    "            if len(winners) == 2:\n",
    "                winners.append([])\n",
    "            winners[2].append(teamName)\n",
    "        if wins > 3:\n",
    "            if len(winners) == 3:\n",
    "                winners.append([])\n",
    "            winners[3].append(teamName)\n",
    "        if wins > 4:\n",
    "            if len(winners) == 4:\n",
    "                winners.append([])\n",
    "            winners[4].append(teamName)\n",
    "        if wins > 5:\n",
    "            if len(winners) == 5:\n",
    "                winners.append([])\n",
    "            winners[5].append(teamName)\n",
    "        if wins > 6:\n",
    "            if len(winners) == 6:\n",
    "                winners.append([])\n",
    "            winners[6].append(teamName)\n",
    "    \n",
    "    return winners"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Method that takes in season's tournament data, id to name mapping of teams\n",
    "def calcBracketScore(teamTeamWinChanceMap, tourney_data):\n",
    "    \n",
    "    winners = getWinnersList(tourney_data)\n",
    "    \n",
    "    # First four rows is 'first four' and not in actual bracket\n",
    "    firstFour = tourney_data[:4]\n",
    "    mainTourney = tourney_data[4:]\n",
    "    \n",
    "    tourney_seeds = pd.read_csv('./ncaa-data/TourneySeeds.csv')\n",
    "    \n",
    "    tourney = [-1] * max(tourney_seeds['Team'])\n",
    "\n",
    "    # Look at first four teams\n",
    "    for index, row in firstFour.iterrows():\n",
    "        index1 = int(row['WTeamID'])\n",
    "        index2 = int(row['LTeamID'])\n",
    "        team1Name = team_id_map[index1]\n",
    "        team2Name = team_id_map[index2]\n",
    "\n",
    "        if (team1Name in teamTeamWinChanceMap) and (team2Name in teamTeamWinChanceMap[team1Name]):\n",
    "            tourney[index2] = index1\n",
    "        else:\n",
    "            tourney[index1] = index2\n",
    "\n",
    "    # MAIN TOURNAMENT \n",
    "    score = 0\n",
    "    for index, row in mainTourney.iterrows():\n",
    "        index1 = int(row['WTeamID'])\n",
    "        index2 = int(row['LTeamID'])\n",
    "        while tourney[index1] > 0:\n",
    "            index1 = tourney[index1]\n",
    "        while tourney[index2] > 0:\n",
    "            index2 = tourney[index2]\n",
    "        team1Name = team_id_map[index1]\n",
    "        team2Name = team_id_map[index2]\n",
    "\n",
    "        if (team1Name in teamTeamWinChanceMap) and (team2Name in teamTeamWinChanceMap[team1Name]): # team1 would win\n",
    "            tourney[index2] = index1\n",
    "            tourney[index1] = tourney[index1] - 1\n",
    "            if team1Name in winners[abs(tourney[index1]) - 2]:\n",
    "                score += 2**(abs(tourney[index1]) - 2) * 10\n",
    "#                 print(team1Name + ' vs ' + team2Name + ', team 1 wins')\n",
    "#                 print('Score ' + str(2**(abs(tourney[index1]) - 2) * 10))\n",
    "#             else: # Delete else statemenet when done debugging\n",
    "#                 print('Incorrect: Predicted ' + team1Name + ' vs ' + team2Name + ', team1 wins')\n",
    "\n",
    "        else: #team2 would win\n",
    "            tourney[index1] = index2\n",
    "            tourney[index2] = tourney[index2] - 1\n",
    "            if team2Name in winners[abs(tourney[index2]) - 2]:\n",
    "                score += 2**(abs(tourney[index2]) - 2) * 10\n",
    "#                 print(team1Name + ' vs ' + team2Name + ', team 2 wins')\n",
    "#                 print('Score ' + str(2**(abs(tourney[index1]) - 2) * 10))\n",
    "#             else: # Delete else statement when done debugging\n",
    "#                 print('Incorrect: Predicted ' + team1Name + ' vs ' + team2Name + ', team2 wins')\n",
    "\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "610\n"
     ]
    }
   ],
   "source": [
    "data_2017 = filterTourneyDataByYear(2017)\n",
    "data_2017\n",
    "print(calcBracketScore(chanceMap, data_2017))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda/envs/py36/lib/python3.6/site-packages/sklearn/decomposition/fastica_.py:118: UserWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.\n",
      "  warnings.warn('FastICA did not converge. Consider increasing '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda/envs/py36/lib/python3.6/site-packages/sklearn/decomposition/fastica_.py:118: UserWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.\n",
      "  warnings.warn('FastICA did not converge. Consider increasing '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda/envs/py36/lib/python3.6/site-packages/sklearn/decomposition/fastica_.py:118: UserWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.\n",
      "  warnings.warn('FastICA did not converge. Consider increasing '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda/envs/py36/lib/python3.6/site-packages/sklearn/decomposition/fastica_.py:118: UserWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.\n",
      "  warnings.warn('FastICA did not converge. Consider increasing '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda/envs/py36/lib/python3.6/site-packages/sklearn/decomposition/fastica_.py:118: UserWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.\n",
      "  warnings.warn('FastICA did not converge. Consider increasing '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda/envs/py36/lib/python3.6/site-packages/sklearn/decomposition/fastica_.py:118: UserWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.\n",
      "  warnings.warn('FastICA did not converge. Consider increasing '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda/envs/py36/lib/python3.6/site-packages/sklearn/decomposition/fastica_.py:118: UserWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.\n",
      "  warnings.warn('FastICA did not converge. Consider increasing '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda/envs/py36/lib/python3.6/site-packages/sklearn/decomposition/fastica_.py:118: UserWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.\n",
      "  warnings.warn('FastICA did not converge. Consider increasing '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda/envs/py36/lib/python3.6/site-packages/sklearn/decomposition/fastica_.py:118: UserWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.\n",
      "  warnings.warn('FastICA did not converge. Consider increasing '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda/envs/py36/lib/python3.6/site-packages/sklearn/decomposition/fastica_.py:118: UserWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.\n",
      "  warnings.warn('FastICA did not converge. Consider increasing '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda/envs/py36/lib/python3.6/site-packages/sklearn/decomposition/fastica_.py:118: UserWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.\n",
      "  warnings.warn('FastICA did not converge. Consider increasing '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda/envs/py36/lib/python3.6/site-packages/sklearn/decomposition/fastica_.py:118: UserWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.\n",
      "  warnings.warn('FastICA did not converge. Consider increasing '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda/envs/py36/lib/python3.6/site-packages/sklearn/decomposition/fastica_.py:118: UserWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.\n",
      "  warnings.warn('FastICA did not converge. Consider increasing '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda/envs/py36/lib/python3.6/site-packages/sklearn/decomposition/fastica_.py:118: UserWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.\n",
      "  warnings.warn('FastICA did not converge. Consider increasing '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda/envs/py36/lib/python3.6/site-packages/sklearn/decomposition/fastica_.py:118: UserWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.\n",
      "  warnings.warn('FastICA did not converge. Consider increasing '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda/envs/py36/lib/python3.6/site-packages/sklearn/decomposition/fastica_.py:118: UserWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.\n",
      "  warnings.warn('FastICA did not converge. Consider increasing '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda/envs/py36/lib/python3.6/site-packages/sklearn/decomposition/fastica_.py:118: UserWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.\n",
      "  warnings.warn('FastICA did not converge. Consider increasing '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda/envs/py36/lib/python3.6/site-packages/sklearn/decomposition/fastica_.py:118: UserWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.\n",
      "  warnings.warn('FastICA did not converge. Consider increasing '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda/envs/py36/lib/python3.6/site-packages/sklearn/decomposition/fastica_.py:118: UserWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.\n",
      "  warnings.warn('FastICA did not converge. Consider increasing '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda/envs/py36/lib/python3.6/site-packages/sklearn/decomposition/fastica_.py:118: UserWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.\n",
      "  warnings.warn('FastICA did not converge. Consider increasing '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda/envs/py36/lib/python3.6/site-packages/sklearn/decomposition/fastica_.py:118: UserWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.\n",
      "  warnings.warn('FastICA did not converge. Consider increasing '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda/envs/py36/lib/python3.6/site-packages/sklearn/decomposition/fastica_.py:118: UserWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.\n",
      "  warnings.warn('FastICA did not converge. Consider increasing '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda/envs/py36/lib/python3.6/site-packages/sklearn/decomposition/fastica_.py:118: UserWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.\n",
      "  warnings.warn('FastICA did not converge. Consider increasing '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda/envs/py36/lib/python3.6/site-packages/sklearn/decomposition/fastica_.py:118: UserWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.\n",
      "  warnings.warn('FastICA did not converge. Consider increasing '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda/envs/py36/lib/python3.6/site-packages/sklearn/decomposition/fastica_.py:118: UserWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.\n",
      "  warnings.warn('FastICA did not converge. Consider increasing '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda/envs/py36/lib/python3.6/site-packages/sklearn/decomposition/fastica_.py:118: UserWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.\n",
      "  warnings.warn('FastICA did not converge. Consider increasing '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda/envs/py36/lib/python3.6/site-packages/sklearn/decomposition/fastica_.py:118: UserWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.\n",
      "  warnings.warn('FastICA did not converge. Consider increasing '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda/envs/py36/lib/python3.6/site-packages/sklearn/decomposition/fastica_.py:118: UserWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.\n",
      "  warnings.warn('FastICA did not converge. Consider increasing '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda/envs/py36/lib/python3.6/site-packages/sklearn/decomposition/fastica_.py:118: UserWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.\n",
      "  warnings.warn('FastICA did not converge. Consider increasing '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-8acc9d37595b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmm_predictor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0manalyze_teams\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maggregated_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     exported_pipeline1 = make_pipeline(\n",
      "\u001b[0;32m~/Desktop/Winter 2018/Info 370 Data Science/info370-FinalProject/mm_predictor.py\u001b[0m in \u001b[0;36manalyze_teams\u001b[0;34m(agg_data)\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m     \u001b[0;31m# Iterate through each row\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgame\u001b[0m \u001b[0;32min\u001b[0m \u001b[0magg_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterrows\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    218\u001b[0m         \u001b[0;31m# Get previous ELOs to update\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m         \u001b[0mwinning_team_elo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_elo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgame\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Season'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgame\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'WTeamID'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/envs/py36/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36miterrows\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    696\u001b[0m         \u001b[0mklass\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_constructor_sliced\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    697\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 698\u001b[0;31m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mklass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    699\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    700\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/envs/py36/lib/python3.6/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, index, dtype, name, copy, fastpath)\u001b[0m\n\u001b[1;32m    248\u001b[0m                                        raise_cast_failure=True)\n\u001b[1;32m    249\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSingleBlockManager\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfastpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m         \u001b[0mgeneric\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNDFrame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfastpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/envs/py36/lib/python3.6/site-packages/pandas/core/internals.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, block, axis, do_integrity_check, fastpath)\u001b[0m\n\u001b[1;32m   4115\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblock\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBlock\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4116\u001b[0m             block = make_block(block, placement=slice(0, len(axis)), ndim=1,\n\u001b[0;32m-> 4117\u001b[0;31m                                fastpath=True)\n\u001b[0m\u001b[1;32m   4118\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4119\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblocks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mblock\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/envs/py36/lib/python3.6/site-packages/pandas/core/internals.py\u001b[0m in \u001b[0;36mmake_block\u001b[0;34m(values, placement, klass, ndim, dtype, fastpath)\u001b[0m\n\u001b[1;32m   2717\u001b[0m                      placement=placement, dtype=dtype)\n\u001b[1;32m   2718\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2719\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mklass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mndim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfastpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfastpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplacement\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mplacement\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2720\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2721\u001b[0m \u001b[0;31m# TODO: flexible with index=None and/or items=None\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/envs/py36/lib/python3.6/site-packages/pandas/core/internals.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, values, ndim, fastpath, placement, **kwargs)\u001b[0m\n\u001b[1;32m   1838\u001b[0m     def __init__(self, values, ndim=2, fastpath=False, placement=None,\n\u001b[1;32m   1839\u001b[0m                  **kwargs):\n\u001b[0;32m-> 1840\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0missubclass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstring_types\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1841\u001b[0m             \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1842\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "data_2017 = filterTourneyDataByYear(2017)\n",
    "\n",
    "for i in range(100):\n",
    "    print(i)\n",
    "    X,Y = mm_predictor.analyze_teams(aggregated_data)\n",
    "    \n",
    "    exported_pipeline1 = make_pipeline(\n",
    "        StackingEstimator(estimator=XGBClassifier(learning_rate=0.01, max_depth=2, min_child_weight=4, n_estimators=100, nthread=1, subsample=0.15000000000000002)),\n",
    "        RobustScaler(),\n",
    "        StackingEstimator(estimator=GaussianNB()),\n",
    "        StackingEstimator(estimator=LinearSVC(C=15.0, dual=True, loss=\"hinge\", penalty=\"l2\", tol=0.0001)),\n",
    "        StackingEstimator(estimator=GaussianNB()),\n",
    "        LogisticRegression(C=5.0, dual=False, penalty=\"l1\")\n",
    "    )\n",
    "    \n",
    "    exported_pipeline1.fit(np.array(X), np.array(Y))\n",
    "    \n",
    "    chanceMap = WinChanceMapping(2017, exported_pipeline1)\n",
    "    \n",
    "    scores1.append(calcBracketScore(chanceMap, data_2017))\n",
    "    \n",
    "    \n",
    "    exported_pipeline2 = make_pipeline(\n",
    "        FastICA(tol=0.0),\n",
    "        FastICA(tol=0.0),\n",
    "        Normalizer(norm=\"l2\"),\n",
    "        GaussianNB()\n",
    "    )\n",
    "\n",
    "    exported_pipeline2.fit(np.array(X), np.array(Y))\n",
    "    chanceMap = WinChanceMapping(2017, exported_pipeline2)\n",
    "    scores2.append(calcBracketScore(chanceMap, data_2017))\n",
    "    \n",
    "print(scores1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[620, 610, 610, 650, 810, 650, 650, 650, 660, 830, 670, 670, 830, 830, 830, 670, 670, 670, 670, 670, 670, 670, 670, 590, 590, 670, 830, 670, 590, 670, 670, 830, 590, 590, 590, 590, 830, 590, 590, 590, 670, 590, 590]\n"
     ]
    }
   ],
   "source": [
    "print(scores1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# From before\n",
    "scores1 = [620, 610, 610, 650, 810, 650, 650, 650, 660, 830, 670, 670, 830, 830, 830, 670, 670, 670, 670, 670, 670, 670, 670, 590, 590, 670, 830, 670, 590, 670, 670, 830, 590, 590, 590, 590, 830, 590, 590, 590, 670, 590, 590]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "670.93023255813955"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(scores1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count     43.000000\n",
       "mean     670.930233\n",
       "std       82.658620\n",
       "min      590.000000\n",
       "25%      590.000000\n",
       "50%      670.000000\n",
       "75%      670.000000\n",
       "max      830.000000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(scores1).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[680, 700, 640, 920, 780, 750, 940, 920, 930, 620, 750, 890, 930, 960, 940, 690, 730, 710, 790, 740, 780, 760, 940, 900, 830, 920, 950, 650, 790, 930, 780, 770, 700, 600, 920, 790, 780, 750, 770, 800, 940, 940, 930]\n"
     ]
    }
   ],
   "source": [
    "print(scores2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# From before\n",
    "scores2 = [680, 700, 640, 920, 780, 750, 940, 920, 930, 620, 750, 890, 930, 960, 940, 690, 730, 710, 790, 740, 780, 760, 940, 900, 830, 920, 950, 650, 790, 930, 780, 770, 700, 600, 920, 790, 780, 750, 770, 800, 940, 940, 930]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "812.32558139534888"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(scores2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count     43.000000\n",
       "mean     812.325581\n",
       "std      106.923141\n",
       "min      600.000000\n",
       "25%      745.000000\n",
       "50%      790.000000\n",
       "75%      925.000000\n",
       "max      960.000000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(scores2).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def bracketScorer(y_true, y_predicted):\n",
    "#     # y_predicted must be a chance map... we don't care what y_true is rofl\n",
    "#     bracket = filterTourneyDataByYear(2017)\n",
    "#     chanceMap = WinChanceMapping(2017)\n",
    "#     return calcBracketScore(chanceMap, bracket)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# from sklearn.metrics import make_scorer\n",
    "\n",
    "# bballScorer = make_scorer(bracketScorer, greater_is_better=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# tppot = TPOTClassifier(generations = 1, max_time_mins=780, verbosity=2, scoring = bballScorer, population_size=10)\n",
    "\n",
    "# tppot.fit(np.array(X), np.array(Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
